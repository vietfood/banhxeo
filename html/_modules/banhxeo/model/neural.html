

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>banhxeo.model.neural &mdash; banhxeo  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            banhxeo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/modules.html">banhxeo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">banhxeo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
          <li class="breadcrumb-item"><a href="../../banhxeo.html">banhxeo</a></li>
      <li class="breadcrumb-item active">banhxeo.model.neural</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for banhxeo.model.neural</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo</span><span class="w"> </span><span class="kn">import</span> <span class="n">CPU_DEVICE</span><span class="p">,</span> <span class="n">GPU_DEVICE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.core.tokenizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">TokenizerConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.core.vocabulary</span><span class="w"> </span><span class="kn">import</span> <span class="n">Vocabulary</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.model.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseLanguageModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.model.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">GenerateConfig</span><span class="p">,</span> <span class="n">ModelConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.utils.logging</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEFAULT_LOGGER</span>


<div class="viewcode-block" id="NeuralModelConfig">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralModelConfig">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">NeuralModelConfig</span><span class="p">(</span><span class="n">ModelConfig</span><span class="p">):</span>  <span class="c1"># noqa: D101</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span></div>



<div class="viewcode-block" id="NeuralLanguageModel">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">NeuralLanguageModel</span><span class="p">(</span><span class="n">BaseLanguageModel</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for neural network-based language models.</span>

<span class="sd">    Extends `BaseLanguageModel` and `torch.nn.Module`. It provides common</span>
<span class="sd">    functionalities for neural models, such as device management (CPU/GPU),</span>
<span class="sd">    model saving/loading (weights and config), attaching downstream heads,</span>
<span class="sd">    and a more detailed summary.</span>

<span class="sd">    Subclasses must implement the `forward` method and typically override</span>
<span class="sd">    `generate_sequence` if applicable.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config (NeuralModelConfig): Configuration specific to neural models,</span>
<span class="sd">            inheriting from `ModelConfig` and often adding `embedding_dim`.</span>
<span class="sd">        vocab (Vocabulary): The vocabulary used by the model.</span>
<span class="sd">        downstream_heads (nn.ModuleDict): A dictionary to hold task-specific</span>
<span class="sd">            output layers (heads) that can be attached to the base model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="NeuralLanguageModel.__init__">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">NeuralModelConfig</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the NeuralLanguageModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_config: The configuration object, instance of `NeuralModelConfig`</span>
<span class="sd">                or its subclass.</span>
<span class="sd">            vocab: The `Vocabulary` instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">BaseLanguageModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>  <span class="c1"># Call LM init first</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># Then nn.Module init</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>  <span class="c1"># Downstream task</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.freeze">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.freeze">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Freezes all parameters of the model.</span>

<span class="sd">        Sets `requires_grad = False` for all parameters, making them</span>
<span class="sd">        non-trainable. Useful for feature extraction or fine-tuning only</span>
<span class="sd">        a part of the model (e.g., a downstream head).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;All parameters in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> have been frozen.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.unfreeze">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.unfreeze">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unfreezes all parameters of the model.</span>

<span class="sd">        Sets `requires_grad = True` for all parameters, making them trainable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;All parameters in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> have been unfrozen.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.summary">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.summary">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prints an enhanced summary of the neural model.</span>

<span class="sd">        Includes the summary from `BaseLanguageModel` and also prints the</span>
<span class="sd">        PyTorch model structure (layers and parameters).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- PyTorch Model Structure ---&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">trainable_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total Parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trainable Parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  Current Device: </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;N/A (No parameters)&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.forward">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.forward">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>  <span class="c1"># More generic for diverse model inputs</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Defines the computation performed at every call.</span>

<span class="sd">        Subclasses must implement this method. It should take tensors as input</span>
<span class="sd">        (e.g., `input_ids`, `attention_mask`) and return a dictionary of</span>
<span class="sd">        output tensors (e.g., `logits`, `hidden_states`, `loss` if computed).</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: Variable length argument list for model inputs.</span>
<span class="sd">            **kwargs: Arbitrary keyword arguments for model inputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary where keys are string names of outputs (e.g., &quot;logits&quot;,</span>
<span class="sd">            &quot;last_hidden_state&quot;) and values are the corresponding `torch.Tensor`s.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the subclass does not implement this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> must implement its own forward pass.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.generate_sequence">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.generate_sequence">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_sequence</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">generate_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerateConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TokenizerConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># For processing prompt</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>  <span class="c1"># For tokenizer.encode during prompt processing</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates a sequence of text starting from a given prompt.</span>

<span class="sd">        This method is typically applicable to autoregressive models (e.g., GPT-like LMs,</span>
<span class="sd">        sequence-to-sequence models in generation mode). Non-autoregressive models</span>
<span class="sd">        (like MLP classifiers or Word2Vec) should raise `NotImplementedError`.</span>

<span class="sd">        The implementation should handle:</span>
<span class="sd">        1. Tokenizing the input `prompt` using `self.vocab.tokenizer`.</span>
<span class="sd">        2. Iteratively predicting the next token.</span>
<span class="sd">        3. Applying sampling strategies specified in `generate_config` (e.g., greedy, top-k).</span>
<span class="sd">        4. Stopping generation based on `max_length` or an end-of-sequence token.</span>
<span class="sd">        5. Detokenizing the generated token IDs back into a string.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: The initial text string to start generation from.</span>
<span class="sd">            generate_config: A `GenerateConfig` object specifying generation</span>
<span class="sd">                parameters like `max_length`, `sampling` strategy, `top_k`, etc.</span>
<span class="sd">                If None, default generation parameters should be used.</span>
<span class="sd">            tokenizer_config: An optional `TokenizerConfig` for encoding the prompt.</span>
<span class="sd">                If None, a default sensible configuration should be used (e.g.,</span>
<span class="sd">                no padding, no truncation initially unless prompt is too long for model).</span>
<span class="sd">            **kwargs: Additional keyword arguments that might be passed to the</span>
<span class="sd">                tokenizer&#39;s `encode` method when processing the prompt.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The generated text string (excluding the initial prompt, or including it,</span>
<span class="sd">            based on implementation choice).</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the model does not support sequence generation.</span>
<span class="sd">            ValueError: If prerequisites for generation (like a tokenizer in vocab) are missing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support sequence generation, &quot;</span>
            <span class="s2">&quot;or it has not been implemented yet.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.attach_downstream_head">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.attach_downstream_head">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">attach_downstream_head</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">head_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Attaches a task-specific head to the base model.</span>

<span class="sd">        This allows reusing the base model&#39;s learned representations for different</span>
<span class="sd">        downstream tasks (e.g., classification, token classification) by adding</span>
<span class="sd">        a new final layer or set of layers.</span>

<span class="sd">        Args:</span>
<span class="sd">            head_name: A unique string name for the head. If a head with this</span>
<span class="sd">                name already exists, it will be replaced.</span>
<span class="sd">            head_module: The `torch.nn.Module` instance representing the head.</span>
<span class="sd">                This head will be registered in `self.downstream_heads`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">head_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replacing existing downstream head: &#39;</span><span class="si">{</span><span class="n">head_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span><span class="p">[</span><span class="n">head_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">head_module</span>
        <span class="c1"># Ensure the new head is moved to the same device as the model</span>
        <span class="n">model_device</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">else</span> <span class="n">CPU_DEVICE</span>
        <span class="p">)</span>
        <span class="n">head_module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Attached downstream head: &#39;</span><span class="si">{</span><span class="n">head_name</span><span class="si">}</span><span class="s2">&#39; (</span><span class="si">{</span><span class="n">head_module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">) &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;to device </span><span class="si">{</span><span class="n">model_device</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.get_downstream_head_output">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.get_downstream_head_output">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_downstream_head_output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">head_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">base_model_output</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">**</span><span class="n">head_kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Passes features from the base model&#39;s output through a specified downstream head.</span>

<span class="sd">        Args:</span>
<span class="sd">            head_name: The name of the downstream head to use (must have been</span>
<span class="sd">                previously attached via `attach_downstream_head`).</span>
<span class="sd">            base_model_output: The dictionary output from the base model&#39;s</span>
<span class="sd">                `forward()` method. The head might expect specific keys from</span>
<span class="sd">                this dictionary (e.g., &quot;last_hidden_state&quot;, &quot;pooled_output&quot;).</span>
<span class="sd">            **head_kwargs: Additional keyword arguments to pass to the</span>
<span class="sd">                downstream head&#39;s `forward` method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The output tensor from the specified downstream head.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If `head_name` is not found in `self.downstream_heads`.</span>
<span class="sd">            ValueError: If the `head_module` (from `self.downstream_heads[head_name]`)</span>
<span class="sd">                is not a callable `nn.Module` or if its `expected_input_key` (if defined)</span>
<span class="sd">                is not in `base_model_output`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">head_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Downstream head &#39;</span><span class="si">{</span><span class="n">head_name</span><span class="si">}</span><span class="s2">&#39; not found. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Available heads: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">head_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downstream_heads</span><span class="p">[</span><span class="n">head_name</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">head_module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
        <span class="p">):</span>  <span class="c1"># Should always be true due to ModuleDict</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Head &#39;</span><span class="si">{</span><span class="n">head_name</span><span class="si">}</span><span class="s2">&#39; is not a valid nn.Module.&quot;</span><span class="p">)</span>

        <span class="c1"># Determine which feature from base_model_output to pass to the head</span>
        <span class="c1"># Some heads might have an &#39;expected_input_key&#39; attribute</span>
        <span class="n">feature_to_pass</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">head_module</span><span class="p">,</span> <span class="s2">&quot;expected_input_key&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">head_module</span><span class="o">.</span><span class="n">expected_input_key</span>
        <span class="p">):</span>
            <span class="n">expected_key</span> <span class="o">=</span> <span class="n">head_module</span><span class="o">.</span><span class="n">expected_input_key</span>
            <span class="k">if</span> <span class="n">expected_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">base_model_output</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Downstream head &#39;</span><span class="si">{</span><span class="n">head_name</span><span class="si">}</span><span class="s2">&#39; expects key &#39;</span><span class="si">{</span><span class="n">expected_key</span><span class="si">}</span><span class="s2">&#39; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;from base model output, but found keys: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">base_model_output</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">feature_to_pass</span> <span class="o">=</span> <span class="n">base_model_output</span><span class="p">[</span><span class="n">expected_key</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Head Module don&#39;t have expected_input_key, cannot feed to downstream head.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">head_module</span><span class="p">(</span><span class="n">feature_to_pass</span><span class="p">,</span> <span class="o">**</span><span class="n">head_kwargs</span><span class="p">)</span></div>


    <span class="c1"># --- Saving and Loading ---</span>
<div class="viewcode-block" id="NeuralLanguageModel.save_model">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.save_model">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Changed from save_path to save_directory</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the neural model&#39;s state_dict, configuration, and vocabulary.</span>

<span class="sd">        The model&#39;s `state_dict` is saved to `pytorch_model.bin` (or similar),</span>
<span class="sd">        the configuration (`self.config`) to `config.json`, and the vocabulary</span>
<span class="sd">        (`self.vocab`) to `vocabulary.json` within the specified `save_directory`.</span>

<span class="sd">        Args:</span>
<span class="sd">            save_directory: The directory path where the model components will be saved.</span>
<span class="sd">                The directory will be created if it doesn&#39;t exist.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">save_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
        <span class="n">save_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># 1. Save model state_dict</span>
        <span class="n">model_save_path</span> <span class="o">=</span> <span class="n">save_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.bin&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">model_save_path</span><span class="p">)</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model state_dict saved to </span><span class="si">{</span><span class="n">model_save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 2. Save model config (self.config should be a Pydantic model)</span>
        <span class="n">config_path</span> <span class="o">=</span> <span class="n">save_dir</span> <span class="o">/</span> <span class="s2">&quot;config.json&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="c1"># Use model_dump for Pydantic models</span>
                <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model configuration saved to </span><span class="si">{</span><span class="n">config_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;An unexpected error occurred while saving model config: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 3. Save vocabulary</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="n">vocab_path</span> <span class="o">=</span> <span class="n">save_dir</span> <span class="o">/</span> <span class="s2">&quot;vocabulary.json&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to save vocabulary to </span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> successfully saved to </span><span class="si">{</span><span class="n">save_dir</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.load_model">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.load_model">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">NeuralLanguageModel</span><span class="p">],</span>
        <span class="n">load_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">],</span>
        <span class="n">vocab</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Vocabulary</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_for_vocab_load</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Tokenizer</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># Needed if vocab is loaded and needs tokenizer</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>  <span class="c1"># For overriding config params or passing to __init__</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralLanguageModel</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads a neural model from a saved directory.</span>

<span class="sd">        This method reconstructs the model by:</span>
<span class="sd">        1. Loading the configuration from `config.json`.</span>
<span class="sd">        2. Loading the vocabulary from `vocabulary.json` (if it exists and `vocab` is not provided).</span>
<span class="sd">        3. Instantiating the model with the loaded config and vocab.</span>
<span class="sd">        4. Loading the saved weights from `pytorch_model.bin` into the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            cls: The specific `NeuralLanguageModel` subclass to instantiate.</span>
<span class="sd">            load_directory: The directory path from which to load the model components.</span>
<span class="sd">            vocab: An optional pre-loaded `Vocabulary` instance. If provided,</span>
<span class="sd">                loading `vocabulary.json` from the directory is skipped.</span>
<span class="sd">            tokenizer_for_vocab_load: A `Tokenizer` instance, required if `vocab`</span>
<span class="sd">                is None and `vocabulary.json` needs to be loaded (as `Vocabulary.load`</span>
<span class="sd">                requires a tokenizer).</span>
<span class="sd">            **model_kwargs: Additional keyword arguments to pass to the model&#39;s</span>
<span class="sd">                `__init__` method, potentially overriding loaded configuration values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An instance of the neural model class, loaded with configuration and weights.</span>

<span class="sd">        Raises:</span>
<span class="sd">            FileNotFoundError: If `config.json` or `pytorch_model.bin` is missing.</span>
<span class="sd">            ValueError: If vocabulary cannot be loaded/provided and is essential.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">load_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">load_directory</span><span class="p">)</span>

        <span class="c1"># 1. Load model config</span>
        <span class="n">config_path</span> <span class="o">=</span> <span class="n">load_dir</span> <span class="o">/</span> <span class="s2">&quot;config.json&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Configuration file &#39;config.json&#39; not found in </span><span class="si">{</span><span class="n">load_dir</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="c1"># Allow overriding loaded config with model_kwargs</span>
        <span class="n">config_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="c1"># Determine which config class to use (subclass might override ConfigClass)</span>
        <span class="n">config_cls</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">ConfigClass</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># Forgivingly remove keys from config_dict not in config_cls for Pydantic</span>
        <span class="n">valid_config_keys</span> <span class="o">=</span> <span class="n">config_cls</span><span class="o">.</span><span class="n">model_fields</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="n">filtered_config_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">valid_config_keys</span>
        <span class="p">}</span>
        <span class="n">unknown_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">config_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="n">valid_config_keys</span>
        <span class="k">if</span> <span class="n">unknown_keys</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Ignoring unknown keys from loaded config for </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">unknown_keys</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">loaded_config</span> <span class="o">=</span> <span class="n">config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">filtered_config_dict</span><span class="p">)</span>

        <span class="c1"># 2. Load or use provided vocabulary</span>
        <span class="n">loaded_vocab_instance</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="k">if</span> <span class="n">loaded_vocab_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">vocab_path</span> <span class="o">=</span> <span class="n">load_dir</span> <span class="o">/</span> <span class="s2">&quot;vocabulary.json&quot;</span>
            <span class="k">if</span> <span class="n">vocab_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">tokenizer_for_vocab_load</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Attempt to infer tokenizer type from config if possible, or use a basic one</span>
                    <span class="c1"># This part is tricky without knowing the original tokenizer</span>
                    <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;tokenizer_for_vocab_load not provided for loading vocabulary.json. &quot;</span>
                        <span class="s2">&quot;Attempting to proceed, but ensure compatibility or provide a Tokenizer.&quot;</span>
                    <span class="p">)</span>
                    <span class="c1"># Fallback: Create a very basic tokenizer for Vocabulary.load</span>
                    <span class="c1"># This might not be the original tokenizer used when vocab was saved.</span>
                    <span class="kn">from</span><span class="w"> </span><span class="nn">banhxeo.core.tokenizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>

                    <span class="n">tokenizer_for_vocab_load</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>

                <span class="n">loaded_vocab_instance</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">vocab_path</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer_for_vocab_load</span>
                <span class="p">)</span>
                <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary loaded from </span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If vocab_size is in config, we might proceed without a full vocab object</span>
                <span class="c1"># if model architecture only needs vocab_size.</span>
                <span class="k">if</span> <span class="n">loaded_config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Vocabulary not provided and &#39;vocabulary.json&#39; not found in </span><span class="si">{</span><span class="n">load_dir</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="s2">&quot;and vocab_size not in config. Cannot initialize model.&quot;</span>
                    <span class="p">)</span>
                <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;vocabulary.json&#39; not found in </span><span class="si">{</span><span class="n">load_dir</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Model will be initialized without a Vocabulary object, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;relying on vocab_size=</span><span class="si">{</span><span class="n">loaded_config</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> from config.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">loaded_vocab_instance</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">loaded_config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Vocabulary is required to load the model, but was not provided or found.&quot;</span>
            <span class="p">)</span>

        <span class="n">init_args</span> <span class="o">=</span> <span class="n">loaded_config</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">loaded_vocab_instance</span><span class="p">,</span> <span class="o">**</span><span class="n">init_args</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># 4. Load model state_dict</span>
        <span class="n">model_path</span> <span class="o">=</span> <span class="n">load_dir</span> <span class="o">/</span> <span class="s2">&quot;pytorch_model.bin&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Model weights file &#39;pytorch_model.bin&#39; not found in </span><span class="si">{</span><span class="n">load_dir</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># Load to CPU first</span>

        <span class="c1"># Handle potential mismatches (e.g. from older versions, missing keys)</span>
        <span class="n">load_result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">load_result</span><span class="o">.</span><span class="n">missing_keys</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Missing keys when loading state_dict: </span><span class="si">{</span><span class="n">load_result</span><span class="o">.</span><span class="n">missing_keys</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">load_result</span><span class="o">.</span><span class="n">unexpected_keys</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unexpected keys when loading state_dict: </span><span class="si">{</span><span class="n">load_result</span><span class="o">.</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">_is_trained_or_fitted</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Assume loaded model was trained</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> loaded successfully from </span><span class="si">{</span><span class="n">load_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.to_gpu">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.to_gpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the model and its parameters to the GPU if available.</span>

<span class="sd">        Checks for CUDA or MPS (Apple Silicon GPU) availability.</span>
<span class="sd">        If no GPU is found, a warning is logged, and the model remains on CPU.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The model itself, now on the GPU (or CPU if no GPU).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">GPU_DEVICE</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;No GPU detected (CUDA or MPS). Model remains on CPU.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">GPU_DEVICE</span><span class="p">)</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> moved to </span><span class="si">{</span><span class="n">GPU_DEVICE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="NeuralLanguageModel.to_cpu">
<a class="viewcode-back" href="../../../api/banhxeo.model.neural.html#banhxeo.model.neural.NeuralLanguageModel.to_cpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the model and its parameters to the CPU.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The model itself, now on the CPU.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">CPU_DEVICE</span><span class="p">)</span>
        <span class="n">DEFAULT_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> moved to </span><span class="si">{</span><span class="n">CPU_DEVICE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Le Nguyen.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>