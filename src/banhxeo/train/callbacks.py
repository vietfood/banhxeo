import shutil

from abc import ABCMeta
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, Optional

from banhxeo.utils import progress_bar
from banhxeo.utils.logging import DEFAULT_LOGGER


# Forward reference
if TYPE_CHECKING:
    from banhxeo.train.trainer import Trainer


class TrainerCallback(metaclass=ABCMeta):
    """Abstract base class for trainer callbacks.

    Callbacks allow custom actions to be performed at various stages of the
    training and evaluation process. Subclasses should override specific
    `on_*` methods to implement their desired behavior.

    Attributes:
        name (str): A unique name for the callback, often used for logging
            or retrieving callback-specific outputs.
    """

    name: str = "base_callback"

    def get_output(self) -> Any:
        """Returns any data accumulated or generated by the callback.

        This method can be overridden by subclasses to provide access to
        callback-specific results (e.g., a list of accuracies, paths to saved models).

        Returns:
            Any data the callback wishes to expose. Defaults to None.
        """
        return None

    def on_init_end(self, trainer: "Trainer") -> None:
        """Called at the end of the `Trainer`'s `__init__` method."""
        pass

    def on_train_begin(self, trainer: "Trainer") -> None:
        """Called at the beginning of the `train` method."""
        pass

    def on_train_end(
        self, trainer: "Trainer", logs: Optional[Dict[str, float]] = None
    ) -> None:
        """Called at the end of the `train` method.

        Args:
            trainer: The `Trainer` instance.
            logs: A dictionary of logs collected at the end of training,
                  such as final average loss.
        """
        pass

    def on_epoch_begin(self, trainer: "Trainer", epoch: int) -> None:
        """Called at the beginning of each training epoch.

        Args:
            trainer: The `Trainer` instance.
            epoch: The current epoch number (1-indexed).
        """
        pass

    def on_epoch_end(
        self, trainer: "Trainer", epoch: int, logs: Optional[Dict[str, float]] = None
    ) -> None:
        """Called at the end of each training epoch.

        Args:
            trainer: The `Trainer` instance.
            epoch: The current epoch number.
            logs: A dictionary of logs for the epoch, such as average epoch loss.
        """
        pass

    def on_step_begin(
        self, trainer: "Trainer", global_step: int, batch_idx: int
    ) -> None:
        """Called at the beginning of each training step (batch processing).

        Args:
            trainer: The `Trainer` instance.
            global_step: The total number of training steps performed so far
                         (optimizer updates if accumulation is handled by trainer,
                         or micro-batches if handled by train_step_fn).
            batch_idx: The index of the current batch within the current epoch.
        """
        pass

    def on_step_end(
        self,
        trainer: "Trainer",
        global_step: int,
        batch_idx: int,
        logs: Optional[Dict[str, Any]] = None,  # logs can contain various metrics
    ) -> None:
        """Called at the end of each training step.

        Args:
            trainer: The `Trainer` instance.
            global_step: The current global step number.
            batch_idx: The index of the current batch.
            logs: A dictionary of logs from the training step, typically
                  including "loss" and any other metrics returned by `train_step_fn`.
        """
        pass

    def on_evaluate(
        self, trainer: "Trainer", metrics: Optional[Dict[str, float]] = None
    ) -> None:
        """Called after an evaluation loop is completed.

        Args:
            trainer: The `Trainer` instance.
            metrics: A dictionary of evaluation metrics (e.g., "eval_loss", "eval_accuracy").
        """
        pass

    def on_save(self, trainer: "Trainer", checkpoint_dir: Path) -> None:
        """Called after a model checkpoint has been saved.

        Args:
            trainer: The `Trainer` instance.
            checkpoint_dir: The path to the directory where the checkpoint was saved.
        """
        pass


class ProgressCallback(TrainerCallback):
    """A callback that displays training progress using `tqdm`."""

    name = "progress"

    def __init__(self) -> None:
        """Initializes the ProgressCallback."""
        super().__init__()
        self.train_pbar = None
        self.eval_pbar = None

    def on_train_begin(self, trainer: "Trainer") -> None:
        """Initializes and displays the training progress bar."""
        if trainer.total_train_steps <= 0:
            DEFAULT_LOGGER.warning(
                "Total train steps is 0 or negative. Progress bar may not function correctly."
            )
        self.train_pbar = progress_bar(
            total=trainer.total_train_steps if trainer.total_train_steps > 0 else None,
            desc=f"Epoch {trainer.current_epoch}/{trainer.config.num_train_epochs} Training",
            unit="step",
            unit_scale=True,
            disable=trainer.total_train_steps <= 0,  # Disable if no steps
        )

    def on_epoch_begin(self, trainer: "Trainer", epoch: int) -> None:
        """Updates the progress bar description for the new epoch."""
        if self.train_pbar:
            self.train_pbar.set_description_str(
                f"Epoch {epoch}/{trainer.config.num_train_epochs} Training"
            )
            # If total_train_steps was for all epochs, reset pbar for current epoch's steps
            # This requires total_train_steps to be per epoch, or a different pbar logic.
            # Current pbar is for total steps across all epochs.

    def on_train_end(
        self, trainer: "Trainer", logs: Optional[Dict[str, float]] = None
    ) -> None:
        """Closes the training progress bar."""
        if self.train_pbar:
            self.train_pbar.close()
            self.train_pbar = None

    def on_step_end(
        self,
        trainer: "Trainer",
        global_step: int,
        batch_idx: int,
        logs: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Updates the training progress bar and sets postfix with current logs."""
        if self.train_pbar:
            self.train_pbar.update(1)
            if logs:
                # Filter logs to display floats/ints for postfix
                display_logs = {
                    k: v for k, v in logs.items() if isinstance(v, (float, int))
                }
                self.train_pbar.set_postfix(display_logs, refresh=True)


class AccuracyCallback(TrainerCallback):
    """A callback to compute and log accuracy during training and evaluation.

    Assumes that the `logs` dictionary passed to `on_step_end` (from `train_step_fn`)
    contains "correct" (number of correctly classified samples in the batch) and
    "total" (total samples in the batch). Similarly for `metrics` in `on_evaluate`.

    Attributes:
        log_step (int): Frequency (in global steps) at which to log training accuracy.
        correct (int): Accumulated count of correct predictions since last log.
        total (int): Accumulated count of total predictions since last log.
        accs (Dict[int, float]): Dictionary storing training accuracies at logged steps.
                                 Key is global_step, value is accuracy percentage.
    """

    name = "accuracy"

    def __init__(self, log_step: int = 100) -> None:
        """Initializes the AccuracyCallback.

        Args:
            log_step: Log training accuracy every `log_step` global steps.
        """
        super().__init__()
        if log_step <= 0:
            raise ValueError("log_step must be positive.")
        self.log_step = log_step
        self.correct: int = 0
        self.total: int = 0
        self.accs: Dict[int, float] = {}  # Stores step: accuracy

    def on_step_end(
        self,
        trainer: "Trainer",
        global_step: int,
        batch_idx: int,
        logs: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Accumulates correct/total counts from step logs and logs training accuracy."""
        if logs is None:
            DEFAULT_LOGGER.warning(
                "AccuracyCallback received None logs at on_step_end. Skipping accuracy update."
            )
            return

        batch_correct = logs.get("correct")
        batch_total = logs.get("total")

        if batch_correct is None or batch_total is None:
            DEFAULT_LOGGER.debug(
                "AccuracyCallback: 'correct' or 'total' key not found in step logs. "
                "Ensure train_step_fn returns these for accuracy calculation."
            )
            return

        if not (isinstance(batch_correct, int) and isinstance(batch_total, int)):
            DEFAULT_LOGGER.warning(
                f"AccuracyCallback: 'correct' ({type(batch_correct)}) and 'total' ({type(batch_total)}) "
                "must be integers. Skipping accuracy update for this step."
            )
            return
        if batch_total < 0:
            DEFAULT_LOGGER.warning(
                f"AccuracyCallback: 'total' samples ({batch_total}) is negative. Skipping."
            )
            return
        if batch_correct < 0 or batch_correct > batch_total:
            DEFAULT_LOGGER.warning(
                f"AccuracyCallback: 'correct' samples ({batch_correct}) is invalid "
                f"for 'total' ({batch_total}). Skipping."
            )
            return

        self.correct += batch_correct
        self.total += batch_total

        # Log step is 1-indexed for user convenience (e.g. every 100th step)
        if (global_step + 1) % self.log_step == 0 and self.total > 0:
            current_acc = (self.correct * 100.0) / self.total
            self.accs[global_step + 1] = current_acc
            DEFAULT_LOGGER.info(
                f"Step {global_step + 1}/{trainer.total_train_steps if trainer.total_train_steps > 0 else '?'}: "
                f"Training Accuracy (last {self.log_step} steps): {current_acc:.2f}% "
                f"({self.correct}/{self.total})"
            )
            # Reset for the next logging interval
            self.correct = 0
            self.total = 0

    def on_evaluate(
        self, trainer: "Trainer", metrics: Optional[Dict[str, float]] = None
    ) -> None:
        """Logs evaluation accuracy from the metrics dictionary."""
        if metrics is None:
            DEFAULT_LOGGER.warning(
                "AccuracyCallback received None metrics at on_evaluate. Skipping."
            )
            return

        eval_correct = metrics.get("correct")
        eval_total = metrics.get("total")

        if eval_correct is not None and eval_total is not None and eval_total > 0:
            acc = (eval_correct * 100.0) / eval_total
            DEFAULT_LOGGER.info(
                f"Evaluation Accuracy: {acc:.2f}% ({int(eval_correct)}/{int(eval_total)})"
            )
        else:
            DEFAULT_LOGGER.debug(
                "AccuracyCallback: 'eval_accuracy' or ('correct' and 'total') "
                "keys not found/valid in evaluation metrics."
            )

    def get_output(self) -> Dict[int, float]:
        """Returns the dictionary of training accuracies recorded at `log_step` intervals."""
        return self.accs


class CheckpointCallback(TrainerCallback):
    """A callback that saves model checkpoints during training.

    Saves checkpoints at specified step intervals (`TrainerConfig.save_steps`)
    and/or at the end of each epoch.
    """

    name = "checkpoint"

    def __init__(self, save_epoch: bool = True):
        """Initializes the CheckpointCallback.

        Args:
            save_epoch: Save at the end of epoch (default = True)
        """
        super().__init__()
        self.save_epoch = save_epoch

    def on_step_end(
        self,
        trainer: "Trainer",
        global_step: int,
        batch_idx: int,
        logs: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Saves a checkpoint if `save_steps` interval is met."""
        current_step_for_saving = global_step + 1
        if (
            trainer.config.save_steps
            and current_step_for_saving % trainer.config.save_steps == 0
            and current_step_for_saving > 0  # Avoid saving at step 0 if save_steps=1
        ):
            self._save_checkpoint(trainer, f"checkpoint-step-{current_step_for_saving}")

    def on_epoch_end(
        self, trainer: "Trainer", epoch: int, logs: Optional[Dict[str, float]] = None
    ) -> None:
        """Saves a checkpoint at the end of an epoch."""
        if self.save_epoch:
            self._save_checkpoint(trainer, f"checkpoint-epoch-{epoch}")

    def _save_checkpoint(self, trainer: "Trainer", checkpoint_name: str) -> None:
        """Internal helper to save a checkpoint.

        Args:
            trainer: The `Trainer` instance.
            checkpoint_name: The name for the checkpoint subdirectory
                (e.g., "checkpoint-step-1000", "checkpoint-epoch-2").
        """
        if not trainer.config.output_dir:
            DEFAULT_LOGGER.warning(
                "output_dir not configured in TrainerConfig. Cannot save checkpoint."
            )
            return

        checkpoint_dir = Path(trainer.config.output_dir) / checkpoint_name
        try:
            trainer.save_model(
                checkpoint_dir
            )  # Trainer.save_model logs success internally
            DEFAULT_LOGGER.info(
                f"Checkpoint '{checkpoint_name}' saved by CheckpointCallback to {checkpoint_dir}"
            )
        except Exception as e:
            DEFAULT_LOGGER.error(
                f"Failed to save checkpoint '{checkpoint_name}' to {checkpoint_dir}: {e}"
            )

        if (
            trainer.config.save_total_limit is not None
            and trainer.config.save_total_limit > 0
        ):
            self._manage_checkpoint_limit(trainer)

    def _manage_checkpoint_limit(self, trainer: "Trainer") -> None:
        """Manages the total number of saved checkpoints, deleting older ones if limit is exceeded."""
        output_dir = Path(trainer.config.output_dir)
        # Get all checkpoint directories, assuming a pattern like "checkpoint-*"
        checkpoints = sorted(
            [
                p
                for p in output_dir.iterdir()
                if p.is_dir() and p.name.startswith("checkpoint-")
            ],
            key=lambda p: p.stat().st_mtime,  # Sort by modification time (oldest first)
        )

        if len(checkpoints) > trainer.config.save_total_limit:  # type: ignore
            num_to_delete = len(checkpoints) - trainer.config.save_total_limit  # type: ignore
            for i in range(num_to_delete):
                dir_to_delete = checkpoints[i]
                try:
                    shutil.rmtree(dir_to_delete)
                    DEFAULT_LOGGER.info(f"Removed old checkpoint: {dir_to_delete}")
                except Exception as e:
                    DEFAULT_LOGGER.error(
                        f"Failed to remove old checkpoint {dir_to_delete}: {e}"
                    )
